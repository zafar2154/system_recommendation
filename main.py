# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wIzN7kPKKmi2XW59LuB7IQN3BehfU9SY

## Import Library Yang Diperlukan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import ast
from google.colab import files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

files.upload()

"""## Loading Data"""

!kaggle datasets download -d tmdb/tmdb-movie-metadata

!unzip tmdb-movie-metadata.zip

movies = pd.read_csv('tmdb_5000_movies.csv')
credits = pd.read_csv('tmdb_5000_credits.csv')

"""## Data Understanding

Pertama-tama lihat dulu info dari masing-masing dataset
"""

credits.info()

movies.info()

"""dataset movie dan credits kita gabung terlebih dahulu"""

movies = movies.merge(credits)

movies.info()

movies.isnull().sum()

movies.duplicated().sum()

jumlah_baris, jumlah_kolom= movies.shape
print("Jumlah baris:", jumlah_baris)
print("Jumlah kolom:", jumlah_kolom)

movies.head()

"""## Data Preprocessing

Dengan kata lain, agar sebuah film dapat tampil di tangga film, film tersebut harus memiliki lebih banyak suara daripada setidaknya 90% film dalam daftar.
"""

m= movies['vote_count'].quantile(0.9)

C = movies['vote_average'].mean()

# Membuat kolom Weighted Rating
def weighted_rating(x, C, m):
    v = x['vote_count']
    R = x['vote_average']
    return (v * R + C * m) / (v + m)

movies['weighted_rating'] = movies.apply(weighted_rating, axis=1, C=C, m=m)

movies.sort_values('weighted_rating', ascending=False).head()

"""Untuk content-based filtering variabel yang digunakan adalah id, title, keywords, genres, cast, dan overview"""

movies = movies[['id','title','keywords','genres','cast','overview','weighted_rating']]

movies.info()

movies.head()

"""dikarenakan variabel keywords, gennres, cast berbentuk JSON yang berisi id dan nama, dan kita hanya butuh namanya saja, maka kita filter variabelnya"""

def extract_names(json_str):
    try:
        keywords = ast.literal_eval(json_str)
        return ",".join([item['name'] for item in keywords])
    except (ValueError, SyntaxError):
        return ""

movies['genres'] = movies['genres'].apply(extract_names)
movies['keywords'] = movies['keywords'].apply(extract_names)
movies['cast'] = movies['cast'].apply(extract_names)

movies.head()

"""dikarenakan pada fungsi extract_names, jika terjadi error saat memproses string JSON (misalnya, format JSON tidak valid), fungsi ini akan mengembalikan string kosong (""), maka kita rubah nilai string kosong menjadi nan agar terdeteksi bahwa string tersebut merupakan missing value dengan fungsi replace()."""

movies = movies.replace('', np.nan)
movies.isnull().sum()

"""terdapat missing value maka kita hapus dengan fungsi dropna()"""

movies=movies.dropna()
movies.isnull().sum()

import itertools

all_genres = list(itertools.chain(*movies['genres'].str.split(',').apply(lambda x: [i.strip() for i in x])))

all_keywords = list(itertools.chain(*movies['keywords'].str.split(',').apply(lambda x: [i.strip() for i in x])))

all_cast = list(itertools.chain(*movies['cast'].str.split(',').apply(lambda x: [i.strip() for i in x])))
# Filter untuk mengecualikan string kosong
unique_genres = set(genre for genre in all_genres if genre)

unique_keywords = set(keyword for keyword in all_keywords if keyword)

unique_cast = set(cast for cast in all_cast if cast)

print("Jumlah genre unik :",len(unique_genres))
print("Jumlah keywords unik :",len(unique_keywords))
print("Jumlah cast unik :",len(unique_cast))

"""membuang spasi pada cast agar nama seperti taylor berbeda antara taylor swift dan taylor kitsch"""

movies['cast'] = movies['cast'].str.replace(' ', '')

"""menggabungkan semua variabel ke 1 variabel features"""

movies['features'] = ' ' + movies['genres'] + ' ' + movies['keywords'] + ' ' + movies['cast'] + ' ' + movies['overview'] + movies['weighted_rating'].astype(str)

"""lowercase text dan mengganti koma dengan spasi"""

movies['features'] = movies['features'].str.lower()
movies['features'] = movies['features'].str.replace(',', ' ')

movies.head()

"""## Modelling

### Modelling dengan TF-IDF
"""

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(movies['features'])

tfidf_matrix.shape

"""Perhatikanlah, matriks yang kita miliki berukuran (4384, 79116). Nilai 4384 merupakan ukuran data dan 79116 merupakan matrik features dari semua film.



"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

cosine_sim

# Membuat dataframe dari variabel cosine_sim
cosine_sim_df = pd.DataFrame(cosine_sim, index=movies['title'], columns=movies['title'])
print('Shape:', cosine_sim_df.shape)

def movies_recommendation(title, similarity_data=cosine_sim_df, items=movies[['title', 'genres', 'cast', 'overview', 'keywords','weighted_rating']], k=10):


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

movies_recommendation("Pirates of the Caribbean: At World's End")

"""### Modelling dengan word2vec"""

movies['tokens'] = movies['features'].apply(lambda x: x.split())

from gensim.models import Word2Vec
model = Word2Vec(sentences=movies['tokens'], vector_size=100, window=5, min_count=1, workers=4, sg=1)

def get_average_vector(tokens, model, vector_size):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(vector_size)
movies['embedding'] = movies['tokens'].apply(lambda x: get_average_vector(x, model, 100))

embeddings = np.array(movies['embedding'].tolist())
similarity_matrix = cosine_similarity(embeddings)

similarity_matrix

sim_df_w2v = pd.DataFrame(similarity_matrix, index=movies['title'], columns=movies['title'])
print('Shape:', sim_df_w2v.shape)

sim_df_w2v.sample(5, axis=1).sample(10, axis=0)

def movies_recommendation_embed(title, similarity_data=sim_df_w2v, items=movies[['title', 'genres', 'cast', 'overview', 'keywords','weighted_rating']], k=15):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

movies_recommendation_embed("Pirates of the Caribbean: At World's End")

"""## Evaluate

evaluasi dengan precision@k
"""

def precision_at_k(recommended_items, relevant_items, k):
    recommended_at_k = recommended_items[:k]
    num_relevant = len(set(recommended_at_k) & set(relevant_items))
    return num_relevant / k

recommended_movies = movies_recommendation("Pirates of the Caribbean: At World's End")['title'].tolist()  # Mendapatkan daftar film yang direkomendasikan
relevant_movies = ['Pirates of the Caribbean: Dead Man\'s Chest', 'Pirates of the Caribbean: The Curse of the Black Pearl','Pirates of the Caribbean: On Stranger Tides',
                   'Pirates of the Caribbean: Dead Men Tell No Tales','Gone in Sixty Seconds','Spider-Man 3',
                   'Shrek the Third', 'Men in Black', 'Harry Potter and the Order of the Phoenix','Ratatouille']  # Daftar film yang relevan
k = 10  # Jumlah item teratas yang dipertimbangkan

precision = precision_at_k(recommended_movies, relevant_movies, k)
print(f"Precision@{k}: {precision}")

def precision_at_k(recommended_items, relevant_items, k):
    recommended_at_k = recommended_items[:k]
    num_relevant = len(set(recommended_at_k) & set(relevant_items))
    return num_relevant / k

recommended_movies_embed = movies_recommendation_embed("Pirates of the Caribbean: At World's End")['title'].tolist()  # Mendapatkan daftar film yang direkomendasikan
relevant_movies = ['Pirates of the Caribbean: Dead Man\'s Chest', 'Pirates of the Caribbean: The Curse of the Black Pearl','Pirates of the Caribbean: On Stranger Tides',
                   'Pirates of the Caribbean: Dead Men Tell No Tales','Gone in Sixty Seconds','Spider-Man 3',
                   'Shrek the Third', 'Men in Black', 'Harry Potter and the Order of the Phoenix','Ratatouille']  # Daftar film yang relevan
k = 10  # Jumlah item teratas yang dipertimbangkan

precision = precision_at_k(recommended_movies_embed, relevant_movies, k)
print(f"Precision@{k}: {precision}")